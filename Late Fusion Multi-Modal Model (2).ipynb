{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e431d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from numpy import loadtxt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a46f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('mouse1sample1.hdf5','r')\n",
    "p_scores = pd.read_csv('merfish_M1S1_filtered_periph_scores.csv')\n",
    "p_scores = p_scores.loc[p_scores['annotation'] != 'unannotated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8925e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # time intensive step: get list of annotated cell_ids and annotation types \n",
    "annotated_cells = []\n",
    "annotations = []\n",
    "for cell_id in f['cells']:\n",
    "    cell = f['cells'][cell_id]\n",
    "    ann = dict(cell.attrs)['annotation']\n",
    "    if ann != 'unannotated' and cell_id in list(p_scores['cell_id']):\n",
    "        annotated_cells.append(cell_id)\n",
    "        annotations.append(ann)\n",
    "        \n",
    "annotations_count = Counter(annotations)\n",
    "print(\"Number of Annotated Samples:\", len(annotated_cells))\n",
    "print(\"Num of Cell Types:\", len(annotations_count))\n",
    "print(\"Max Number of Samples per Cell Type:\", max(annotations_count.values()))\n",
    "print(\"Min Number of Samples per Cell Type:\", min(annotations_count.values()))\n",
    "print(\"Average Number of Samples per Cell Type:\", sum(annotations_count.values())/93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bbac2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "periphery data: shape (len(annotated_cells), num_genes)\n",
    "num_genes = (p_scores['gene']).unique().shape[0]\n",
    "cell_indices = {k: v for v, k in enumerate(list(p_scores['cell_id'].unique()))}\n",
    "gene_indices = {k: v for v, k in enumerate(list(p_scores['gene'].unique()))}\n",
    "\n",
    "periphery_data = np.zeros((len(annotated_cells), num_genes))\n",
    "for cell_id in annotated_cells:\n",
    "    subset = p_scores.loc[p_scores['cell_id'] == cell_id]\n",
    "    for index, s in subset.iterrows():\n",
    "        cell_index = cell_indices[cell_id]\n",
    "        gene_index = gene_indices[s['gene']]\n",
    "        periphery_data[cell_index][gene_index] = s['periphery_score']\n",
    "\n",
    "# remove outlier genes\n",
    "expr = np.sum(np.square(periphery_data), axis=0)\n",
    "periphery_data = periphery_data[:,np.logical_and(expr >= np.percentile(expr, 1), expr <= np.percentile(expr, 99))]\n",
    "cv = np.std(periphery_data, axis=0) / np.mean(periphery_data, axis=0)\n",
    "periphery_data = periphery_data[:,np.logical_and(cv >= np.percentile(cv, 1), cv <= np.percentile(cv, 99))]\n",
    "np.save('periphery_data', periphery_data)\n",
    "\n",
    "periphery_data = np.load('periphery_data.npy')\n",
    "\n",
    "# remove outlier genes\n",
    "expr = np.sum(np.square(periphery_data), axis=0)\n",
    "periphery_data = periphery_data[:,np.logical_and(expr >= np.percentile(expr, 1), expr <= np.percentile(expr, 99))]\n",
    "cv = np.std(periphery_data, axis=0) / np.mean(periphery_data, axis=0)\n",
    "periphery_data = periphery_data[:,np.logical_and(cv >= np.percentile(cv, 1), cv <= np.percentile(cv, 99))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5305c3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #image data: shape (len(annotated_cells), 224, 224))\n",
    "image_data = np.zeros(shape=(len(annotated_cells), 224, 224)).astype('uint8')\n",
    "fig = plt.figure(figsize=(4, 4), dpi=56)\n",
    "plt.gray()\n",
    "idx = 0\n",
    "for cell_id in annotated_cells:\n",
    "    cell = f['cells'][cell_id]\n",
    "    keys = list(cell['boundaries'].keys())\n",
    "    midpoint = keys[int(len(keys)/2)]\n",
    "    boundary = cell['boundaries'][midpoint]\n",
    "    xs = boundary[:,0]\n",
    "    ys = boundary[:,1]\n",
    "    plt.plot(xs,ys)\n",
    "    plt.axis('equal')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    fig.canvas.draw()\n",
    "    fig_array = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    fig_array = fig_array.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    image_data[idx] = fig_array[:,:,0].astype('uint8')\n",
    "    idx += 1\n",
    "    plt.clf()\n",
    "    print(f\"\\r{idx}\", end=\"\")\n",
    "  \n",
    "print(image_data.shape)\n",
    "np.save('image_data', image_data)\n",
    "\n",
    "image_data = np.load('image_data.npy')\n",
    "image_data = np.repeat(image_data[..., np.newaxis], 3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5540119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17312, 80)\n"
     ]
    }
   ],
   "source": [
    "# one hot encoded label vectors\n",
    "annotation_indices = {k: v for v, k in enumerate(list(annotations_count.keys()))}\n",
    "labels = np.zeros((len(annotated_cells), len(annotation_indices)))\n",
    "for index in range(0, len(annotated_cells)):\n",
    "    cell = f['cells'][annotated_cells[index]]\n",
    "    ann = dict(cell.attrs)['annotation']\n",
    "    arr_index = annotation_indices[ann]\n",
    "    labels[index][arr_index] = 1\n",
    "print(labels.shape)\n",
    "np.save('labels', labels)\n",
    "\n",
    "labels = np.load('labels.npy')\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f371a035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12118, 224, 224, 3) (12118, 184) (12118, 80)\n",
      "(2597, 224, 224, 3) (2597, 184) (2597, 80)\n",
      "(2597, 224, 224, 3) (2597, 184) (2597, 80)\n"
     ]
    }
   ],
   "source": [
    "indices = [x for x in range(labels.shape[0])]\n",
    "train_indices, test = train_test_split(indices, test_size=0.3)\n",
    "test_indices, val_indices = train_test_split(test, test_size=0.5)\n",
    "\n",
    "image_train, periphery_train, labels_train = image_data[train_indices], periphery_data[train_indices], labels[train_indices]\n",
    "image_test, periphery_test, labels_test = image_data[test_indices], periphery_data[test_indices], labels[test_indices]\n",
    "image_val, periphery_val, labels_val = image_data[val_indices], periphery_data[val_indices], labels[val_indices]\n",
    "\n",
    "print(image_train.shape, periphery_train.shape, labels_train.shape)\n",
    "print(image_test.shape, periphery_test.shape, labels_test.shape)\n",
    "print(image_val.shape, periphery_val.shape, labels_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63520aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 22:32:18.781057: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-09 22:32:18.894667: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-09 22:32:18.895569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-09 22:32:18.897141: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-09 22:32:18.897733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-09 22:32:18.898607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-09 22:32:18.899505: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-09 22:32:20.928080: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-09 22:32:20.928933: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-09 22:32:20.929726: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-12-09 22:32:20.931586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10819 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\n"
     ]
    }
   ],
   "source": [
    "# initializing individual models + late fusion model\n",
    "class WeightedAverage(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, n_output):\n",
    "        super(WeightedAverage, self).__init__()\n",
    "        self.W = tf.Variable(initial_value=tf.random.uniform(shape=[1,1,n_output], minval=0, maxval=1),\n",
    "            trainable=True) # (1,1,n_inputs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        # inputs is a list of tensor of shape [(n_batch, n_feat), ..., (n_batch, n_feat)]\n",
    "        # expand last dim of each input passed [(n_batch, n_feat, 1), ..., (n_batch, n_feat, 1)]\n",
    "        inputs = [tf.expand_dims(i, -1) for i in inputs]\n",
    "        inputs = tf.keras.layers.concatenate(axis=-1)(inputs) # (n_batch, n_feat, n_inputs)\n",
    "        weights = tf.nn.softmax(self.W, axis=-1) # (1,1,n_inputs)\n",
    "        # weights sum up to one on last dim\n",
    "        return tf.reduce_sum(weights*inputs, axis=-1) # (n_batch, n_feat)\n",
    "    \n",
    "periphery_model = tf.keras.models.Sequential([tf.keras.layers.Input(periphery_data.shape[1]),\n",
    "                                    tf.keras.layers.Dense(100, activation=tf.nn.relu), \n",
    "                                    tf.keras.layers.Dense(50, activation=tf.nn.relu), \n",
    "                                    tf.keras.layers.Dense(25, activation=tf.nn.relu), \n",
    "                                    tf.keras.layers.Dense(80, activation=tf.nn.softmax)])\n",
    "\n",
    "image_model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(32,(3,3),padding='same', activation='relu',input_shape=(224, 224, 3)),\n",
    "                                    tf.keras.layers.MaxPool2D((2,2)), \n",
    "                                    tf.keras.layers.Flatten(), \n",
    "                                    tf.keras.layers.Dropout(0.9),\n",
    "                                    tf.keras.layers.Dense(64, activation=tf.nn.relu), \n",
    "                                    tf.keras.layers.Dense(80, activation=tf.nn.softmax)])\n",
    "\n",
    "# model = tf.keras.models.Sequential()\n",
    "# model.add(WeightedAverage(periphery_model.output, image_model.output))\n",
    "# # weighted_average = tf.keras.layers.concatenate([tf.multiply(periphery_model.output, 0.1), tf.multiply(image_model.output, 0.1)], axis=0)\n",
    "# # WA = WeightedAverage(n_output = 2)([periphery_model.output, image_model.output])\n",
    "# # model = tf.keras.models.Model(inputs=[periphery_model.input, image_model.input], outputs = WA)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7abbc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compiling late fusion model with exponential decay lr\n",
    "lr= tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    0.0001,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.95,\n",
    "    staircase=True)\n",
    "\n",
    "# early stopping to prevent overfitting \n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True)\n",
    "periphery_model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(learning_rate=lr), metrics=['accuracy', tf.keras.metrics.AUC(curve='PR', multi_label=True, num_labels=80)])\n",
    "# image_model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37c1a439",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 22:32:21.296303: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "379/379 [==============================] - 4s 6ms/step - loss: 4.3242 - accuracy: 0.0300 - auc: 0.0132 - val_loss: 4.1716 - val_accuracy: 0.0431 - val_auc: 0.0157\n",
      "Epoch 2/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 3.7468 - accuracy: 0.0977 - auc: 0.0156 - val_loss: 3.5013 - val_accuracy: 0.1201 - val_auc: 0.0189\n",
      "Epoch 3/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 3.4676 - accuracy: 0.1169 - auc: 0.0221 - val_loss: 3.4404 - val_accuracy: 0.1302 - val_auc: 0.0227\n",
      "Epoch 4/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 3.4161 - accuracy: 0.1382 - auc: 0.0275 - val_loss: 3.4024 - val_accuracy: 0.1479 - val_auc: 0.0284\n",
      "Epoch 5/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 3.3660 - accuracy: 0.1813 - auc: 0.0333 - val_loss: 3.3543 - val_accuracy: 0.2029 - val_auc: 0.0342\n",
      "Epoch 6/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 3.3091 - accuracy: 0.2125 - auc: 0.0382 - val_loss: 3.3036 - val_accuracy: 0.2087 - val_auc: 0.0411\n",
      "Epoch 7/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 3.2491 - accuracy: 0.2207 - auc: 0.0428 - val_loss: 3.2516 - val_accuracy: 0.2156 - val_auc: 0.0432\n",
      "Epoch 8/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 3.1907 - accuracy: 0.2297 - auc: 0.0471 - val_loss: 3.2011 - val_accuracy: 0.2245 - val_auc: 0.0453\n",
      "Epoch 9/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 3.1346 - accuracy: 0.2351 - auc: 0.0510 - val_loss: 3.1507 - val_accuracy: 0.2314 - val_auc: 0.0493\n",
      "Epoch 10/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 3.0780 - accuracy: 0.2404 - auc: 0.0548 - val_loss: 3.0957 - val_accuracy: 0.2372 - val_auc: 0.0520\n",
      "Epoch 11/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 3.0176 - accuracy: 0.2459 - auc: 0.0600 - val_loss: 3.0396 - val_accuracy: 0.2445 - val_auc: 0.0565\n",
      "Epoch 12/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.9601 - accuracy: 0.2507 - auc: 0.0618 - val_loss: 2.9869 - val_accuracy: 0.2464 - val_auc: 0.0596\n",
      "Epoch 13/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.9062 - accuracy: 0.2595 - auc: 0.0657 - val_loss: 2.9387 - val_accuracy: 0.2541 - val_auc: 0.0618\n",
      "Epoch 14/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.8580 - accuracy: 0.2688 - auc: 0.0691 - val_loss: 2.8994 - val_accuracy: 0.2622 - val_auc: 0.0655\n",
      "Epoch 15/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.8162 - accuracy: 0.2764 - auc: 0.0719 - val_loss: 2.8632 - val_accuracy: 0.2715 - val_auc: 0.0666\n",
      "Epoch 16/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.7784 - accuracy: 0.2885 - auc: 0.0748 - val_loss: 2.8317 - val_accuracy: 0.2784 - val_auc: 0.0704\n",
      "Epoch 17/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.7448 - accuracy: 0.2951 - auc: 0.0774 - val_loss: 2.8037 - val_accuracy: 0.2873 - val_auc: 0.0704\n",
      "Epoch 18/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.7141 - accuracy: 0.3013 - auc: 0.0786 - val_loss: 2.7784 - val_accuracy: 0.2903 - val_auc: 0.0725\n",
      "Epoch 19/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.6850 - accuracy: 0.3071 - auc: 0.0813 - val_loss: 2.7543 - val_accuracy: 0.2926 - val_auc: 0.0756\n",
      "Epoch 20/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.6585 - accuracy: 0.3146 - auc: 0.0832 - val_loss: 2.7323 - val_accuracy: 0.3011 - val_auc: 0.0783\n",
      "Epoch 21/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.6329 - accuracy: 0.3209 - auc: 0.0860 - val_loss: 2.7107 - val_accuracy: 0.3057 - val_auc: 0.0801\n",
      "Epoch 22/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.6087 - accuracy: 0.3261 - auc: 0.0876 - val_loss: 2.6908 - val_accuracy: 0.3088 - val_auc: 0.0807\n",
      "Epoch 23/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.5858 - accuracy: 0.3294 - auc: 0.0891 - val_loss: 2.6720 - val_accuracy: 0.3127 - val_auc: 0.0826\n",
      "Epoch 24/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.5632 - accuracy: 0.3343 - auc: 0.0903 - val_loss: 2.6531 - val_accuracy: 0.3177 - val_auc: 0.0857\n",
      "Epoch 25/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.5416 - accuracy: 0.3416 - auc: 0.0924 - val_loss: 2.6352 - val_accuracy: 0.3208 - val_auc: 0.0869\n",
      "Epoch 26/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.5205 - accuracy: 0.3436 - auc: 0.0940 - val_loss: 2.6173 - val_accuracy: 0.3238 - val_auc: 0.0908\n",
      "Epoch 27/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.4997 - accuracy: 0.3480 - auc: 0.0975 - val_loss: 2.5997 - val_accuracy: 0.3288 - val_auc: 0.0918\n",
      "Epoch 28/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.4798 - accuracy: 0.3510 - auc: 0.0985 - val_loss: 2.5836 - val_accuracy: 0.3327 - val_auc: 0.0936\n",
      "Epoch 29/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.4605 - accuracy: 0.3548 - auc: 0.1003 - val_loss: 2.5665 - val_accuracy: 0.3338 - val_auc: 0.0935\n",
      "Epoch 30/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.4414 - accuracy: 0.3582 - auc: 0.1021 - val_loss: 2.5503 - val_accuracy: 0.3381 - val_auc: 0.0947\n",
      "Epoch 31/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.4228 - accuracy: 0.3632 - auc: 0.1041 - val_loss: 2.5354 - val_accuracy: 0.3404 - val_auc: 0.0984\n",
      "Epoch 32/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.4048 - accuracy: 0.3666 - auc: 0.1057 - val_loss: 2.5196 - val_accuracy: 0.3466 - val_auc: 0.1000\n",
      "Epoch 33/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.3872 - accuracy: 0.3695 - auc: 0.1076 - val_loss: 2.5045 - val_accuracy: 0.3489 - val_auc: 0.1008\n",
      "Epoch 34/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.3698 - accuracy: 0.3742 - auc: 0.1098 - val_loss: 2.4899 - val_accuracy: 0.3543 - val_auc: 0.1062\n",
      "Epoch 35/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.3527 - accuracy: 0.3779 - auc: 0.1110 - val_loss: 2.4755 - val_accuracy: 0.3585 - val_auc: 0.1078\n",
      "Epoch 36/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.3360 - accuracy: 0.3822 - auc: 0.1133 - val_loss: 2.4617 - val_accuracy: 0.3593 - val_auc: 0.1140\n",
      "Epoch 37/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.3197 - accuracy: 0.3841 - auc: 0.1150 - val_loss: 2.4469 - val_accuracy: 0.3643 - val_auc: 0.1146\n",
      "Epoch 38/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.3035 - accuracy: 0.3886 - auc: 0.1178 - val_loss: 2.4335 - val_accuracy: 0.3673 - val_auc: 0.1158\n",
      "Epoch 39/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.2878 - accuracy: 0.3917 - auc: 0.1181 - val_loss: 2.4196 - val_accuracy: 0.3689 - val_auc: 0.1161\n",
      "Epoch 40/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.2724 - accuracy: 0.3935 - auc: 0.1216 - val_loss: 2.4070 - val_accuracy: 0.3720 - val_auc: 0.1179\n",
      "Epoch 41/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.2575 - accuracy: 0.3976 - auc: 0.1229 - val_loss: 2.3942 - val_accuracy: 0.3716 - val_auc: 0.1126\n",
      "Epoch 42/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.2427 - accuracy: 0.3992 - auc: 0.1242 - val_loss: 2.3818 - val_accuracy: 0.3777 - val_auc: 0.1136\n",
      "Epoch 43/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.2288 - accuracy: 0.4025 - auc: 0.1258 - val_loss: 2.3707 - val_accuracy: 0.3781 - val_auc: 0.1145\n",
      "Epoch 44/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.2150 - accuracy: 0.4058 - auc: 0.1269 - val_loss: 2.3589 - val_accuracy: 0.3804 - val_auc: 0.1178\n",
      "Epoch 45/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.2018 - accuracy: 0.4076 - auc: 0.1284 - val_loss: 2.3474 - val_accuracy: 0.3854 - val_auc: 0.1186\n",
      "Epoch 46/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.1885 - accuracy: 0.4102 - auc: 0.1307 - val_loss: 2.3369 - val_accuracy: 0.3862 - val_auc: 0.1180\n",
      "Epoch 47/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.1761 - accuracy: 0.4115 - auc: 0.1327 - val_loss: 2.3261 - val_accuracy: 0.3866 - val_auc: 0.1195\n",
      "Epoch 48/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.1638 - accuracy: 0.4153 - auc: 0.1341 - val_loss: 2.3160 - val_accuracy: 0.3870 - val_auc: 0.1216\n",
      "Epoch 49/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.1519 - accuracy: 0.4161 - auc: 0.1354 - val_loss: 2.3061 - val_accuracy: 0.3912 - val_auc: 0.1215\n",
      "Epoch 50/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.1404 - accuracy: 0.4190 - auc: 0.1367 - val_loss: 2.2962 - val_accuracy: 0.3928 - val_auc: 0.1230\n",
      "Epoch 51/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.1290 - accuracy: 0.4203 - auc: 0.1376 - val_loss: 2.2870 - val_accuracy: 0.3943 - val_auc: 0.1230\n",
      "Epoch 52/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.1182 - accuracy: 0.4226 - auc: 0.1401 - val_loss: 2.2784 - val_accuracy: 0.3966 - val_auc: 0.1234\n",
      "Epoch 53/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.1076 - accuracy: 0.4252 - auc: 0.1417 - val_loss: 2.2694 - val_accuracy: 0.3978 - val_auc: 0.1240\n",
      "Epoch 54/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.0973 - accuracy: 0.4282 - auc: 0.1436 - val_loss: 2.2612 - val_accuracy: 0.3966 - val_auc: 0.1269\n",
      "Epoch 55/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.0876 - accuracy: 0.4309 - auc: 0.1442 - val_loss: 2.2531 - val_accuracy: 0.3985 - val_auc: 0.1275\n",
      "Epoch 56/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.0777 - accuracy: 0.4331 - auc: 0.1454 - val_loss: 2.2450 - val_accuracy: 0.4016 - val_auc: 0.1288\n",
      "Epoch 57/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.0682 - accuracy: 0.4356 - auc: 0.1469 - val_loss: 2.2378 - val_accuracy: 0.4035 - val_auc: 0.1303\n",
      "Epoch 58/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.0592 - accuracy: 0.4371 - auc: 0.1481 - val_loss: 2.2300 - val_accuracy: 0.4039 - val_auc: 0.1320\n",
      "Epoch 59/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.0504 - accuracy: 0.4393 - auc: 0.1498 - val_loss: 2.2231 - val_accuracy: 0.4082 - val_auc: 0.1315\n",
      "Epoch 60/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.0419 - accuracy: 0.4398 - auc: 0.1509 - val_loss: 2.2165 - val_accuracy: 0.4151 - val_auc: 0.1308\n",
      "Epoch 61/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.0336 - accuracy: 0.4448 - auc: 0.1513 - val_loss: 2.2096 - val_accuracy: 0.4170 - val_auc: 0.1330\n",
      "Epoch 62/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.0254 - accuracy: 0.4471 - auc: 0.1530 - val_loss: 2.2028 - val_accuracy: 0.4193 - val_auc: 0.1336\n",
      "Epoch 63/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.0178 - accuracy: 0.4528 - auc: 0.1535 - val_loss: 2.1968 - val_accuracy: 0.4228 - val_auc: 0.1339\n",
      "Epoch 64/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.0099 - accuracy: 0.4524 - auc: 0.1550 - val_loss: 2.1906 - val_accuracy: 0.4224 - val_auc: 0.1333\n",
      "Epoch 65/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 2.0027 - accuracy: 0.4563 - auc: 0.1559 - val_loss: 2.1848 - val_accuracy: 0.4247 - val_auc: 0.1336\n",
      "Epoch 66/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.9955 - accuracy: 0.4599 - auc: 0.1562 - val_loss: 2.1788 - val_accuracy: 0.4274 - val_auc: 0.1345\n",
      "Epoch 67/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.9883 - accuracy: 0.4617 - auc: 0.1575 - val_loss: 2.1731 - val_accuracy: 0.4297 - val_auc: 0.1374\n",
      "Epoch 68/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.9817 - accuracy: 0.4644 - auc: 0.1588 - val_loss: 2.1679 - val_accuracy: 0.4297 - val_auc: 0.1378\n",
      "Epoch 69/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.9751 - accuracy: 0.4657 - auc: 0.1600 - val_loss: 2.1627 - val_accuracy: 0.4317 - val_auc: 0.1386\n",
      "Epoch 70/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.9686 - accuracy: 0.4673 - auc: 0.1604 - val_loss: 2.1579 - val_accuracy: 0.4336 - val_auc: 0.1399\n",
      "Epoch 71/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.9624 - accuracy: 0.4692 - auc: 0.1617 - val_loss: 2.1528 - val_accuracy: 0.4343 - val_auc: 0.1406\n",
      "Epoch 72/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.9563 - accuracy: 0.4719 - auc: 0.1625 - val_loss: 2.1479 - val_accuracy: 0.4370 - val_auc: 0.1405\n",
      "Epoch 73/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.9504 - accuracy: 0.4734 - auc: 0.1635 - val_loss: 2.1431 - val_accuracy: 0.4351 - val_auc: 0.1406\n",
      "Epoch 74/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.9446 - accuracy: 0.4752 - auc: 0.1637 - val_loss: 2.1387 - val_accuracy: 0.4363 - val_auc: 0.1427\n",
      "Epoch 75/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.9390 - accuracy: 0.4756 - auc: 0.1652 - val_loss: 2.1344 - val_accuracy: 0.4370 - val_auc: 0.1429\n",
      "Epoch 76/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.9336 - accuracy: 0.4777 - auc: 0.1658 - val_loss: 2.1300 - val_accuracy: 0.4374 - val_auc: 0.1433\n",
      "Epoch 77/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.9283 - accuracy: 0.4785 - auc: 0.1668 - val_loss: 2.1261 - val_accuracy: 0.4405 - val_auc: 0.1431\n",
      "Epoch 78/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.9232 - accuracy: 0.4797 - auc: 0.1681 - val_loss: 2.1222 - val_accuracy: 0.4413 - val_auc: 0.1436\n",
      "Epoch 79/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.9182 - accuracy: 0.4809 - auc: 0.1692 - val_loss: 2.1182 - val_accuracy: 0.4428 - val_auc: 0.1441\n",
      "Epoch 80/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.9132 - accuracy: 0.4833 - auc: 0.1699 - val_loss: 2.1145 - val_accuracy: 0.4432 - val_auc: 0.1444\n",
      "Epoch 81/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.9084 - accuracy: 0.4860 - auc: 0.1711 - val_loss: 2.1108 - val_accuracy: 0.4440 - val_auc: 0.1447\n",
      "Epoch 82/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.9039 - accuracy: 0.4865 - auc: 0.1724 - val_loss: 2.1074 - val_accuracy: 0.4432 - val_auc: 0.1454\n",
      "Epoch 83/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8994 - accuracy: 0.4874 - auc: 0.1726 - val_loss: 2.1043 - val_accuracy: 0.4424 - val_auc: 0.1463\n",
      "Epoch 84/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8950 - accuracy: 0.4891 - auc: 0.1729 - val_loss: 2.1007 - val_accuracy: 0.4440 - val_auc: 0.1466\n",
      "Epoch 85/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8907 - accuracy: 0.4887 - auc: 0.1741 - val_loss: 2.0972 - val_accuracy: 0.4447 - val_auc: 0.1465\n",
      "Epoch 86/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8865 - accuracy: 0.4903 - auc: 0.1742 - val_loss: 2.0943 - val_accuracy: 0.4451 - val_auc: 0.1469\n",
      "Epoch 87/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8824 - accuracy: 0.4908 - auc: 0.1753 - val_loss: 2.0909 - val_accuracy: 0.4474 - val_auc: 0.1476\n",
      "Epoch 88/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8784 - accuracy: 0.4923 - auc: 0.1757 - val_loss: 2.0880 - val_accuracy: 0.4482 - val_auc: 0.1481\n",
      "Epoch 89/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8746 - accuracy: 0.4927 - auc: 0.1762 - val_loss: 2.0854 - val_accuracy: 0.4490 - val_auc: 0.1483\n",
      "Epoch 90/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8708 - accuracy: 0.4934 - auc: 0.1773 - val_loss: 2.0825 - val_accuracy: 0.4490 - val_auc: 0.1488\n",
      "Epoch 91/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8671 - accuracy: 0.4939 - auc: 0.1776 - val_loss: 2.0802 - val_accuracy: 0.4494 - val_auc: 0.1493\n",
      "Epoch 92/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8635 - accuracy: 0.4959 - auc: 0.1788 - val_loss: 2.0773 - val_accuracy: 0.4490 - val_auc: 0.1493\n",
      "Epoch 93/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8601 - accuracy: 0.4954 - auc: 0.1791 - val_loss: 2.0746 - val_accuracy: 0.4501 - val_auc: 0.1498\n",
      "Epoch 94/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8566 - accuracy: 0.4965 - auc: 0.1794 - val_loss: 2.0722 - val_accuracy: 0.4509 - val_auc: 0.1508\n",
      "Epoch 95/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8532 - accuracy: 0.4969 - auc: 0.1800 - val_loss: 2.0698 - val_accuracy: 0.4513 - val_auc: 0.1510\n",
      "Epoch 96/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8500 - accuracy: 0.4968 - auc: 0.1805 - val_loss: 2.0674 - val_accuracy: 0.4528 - val_auc: 0.1506\n",
      "Epoch 97/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8469 - accuracy: 0.4970 - auc: 0.1807 - val_loss: 2.0651 - val_accuracy: 0.4528 - val_auc: 0.1509\n",
      "Epoch 98/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8438 - accuracy: 0.4994 - auc: 0.1822 - val_loss: 2.0628 - val_accuracy: 0.4532 - val_auc: 0.1508\n",
      "Epoch 99/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8407 - accuracy: 0.4996 - auc: 0.1822 - val_loss: 2.0607 - val_accuracy: 0.4536 - val_auc: 0.1505\n",
      "Epoch 100/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8378 - accuracy: 0.4994 - auc: 0.1823 - val_loss: 2.0586 - val_accuracy: 0.4532 - val_auc: 0.1505\n",
      "Epoch 101/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8349 - accuracy: 0.5013 - auc: 0.1822 - val_loss: 2.0564 - val_accuracy: 0.4532 - val_auc: 0.1507\n",
      "Epoch 102/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8320 - accuracy: 0.5008 - auc: 0.1834 - val_loss: 2.0543 - val_accuracy: 0.4536 - val_auc: 0.1507\n",
      "Epoch 103/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8293 - accuracy: 0.5015 - auc: 0.1834 - val_loss: 2.0522 - val_accuracy: 0.4532 - val_auc: 0.1511\n",
      "Epoch 104/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8265 - accuracy: 0.5025 - auc: 0.1846 - val_loss: 2.0504 - val_accuracy: 0.4540 - val_auc: 0.1512\n",
      "Epoch 105/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8240 - accuracy: 0.5030 - auc: 0.1852 - val_loss: 2.0483 - val_accuracy: 0.4540 - val_auc: 0.1517\n",
      "Epoch 106/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8214 - accuracy: 0.5036 - auc: 0.1855 - val_loss: 2.0466 - val_accuracy: 0.4540 - val_auc: 0.1519\n",
      "Epoch 107/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8188 - accuracy: 0.5036 - auc: 0.1854 - val_loss: 2.0449 - val_accuracy: 0.4555 - val_auc: 0.1528\n",
      "Epoch 108/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8164 - accuracy: 0.5051 - auc: 0.1862 - val_loss: 2.0431 - val_accuracy: 0.4555 - val_auc: 0.1527\n",
      "Epoch 109/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8140 - accuracy: 0.5049 - auc: 0.1866 - val_loss: 2.0414 - val_accuracy: 0.4548 - val_auc: 0.1529\n",
      "Epoch 110/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8117 - accuracy: 0.5052 - auc: 0.1870 - val_loss: 2.0398 - val_accuracy: 0.4548 - val_auc: 0.1547\n",
      "Epoch 111/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8093 - accuracy: 0.5055 - auc: 0.1870 - val_loss: 2.0381 - val_accuracy: 0.4559 - val_auc: 0.1544\n",
      "Epoch 112/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8071 - accuracy: 0.5061 - auc: 0.1870 - val_loss: 2.0366 - val_accuracy: 0.4555 - val_auc: 0.1544\n",
      "Epoch 113/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8049 - accuracy: 0.5063 - auc: 0.1877 - val_loss: 2.0349 - val_accuracy: 0.4567 - val_auc: 0.1546\n",
      "Epoch 114/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8027 - accuracy: 0.5071 - auc: 0.1887 - val_loss: 2.0335 - val_accuracy: 0.4571 - val_auc: 0.1544\n",
      "Epoch 115/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.8007 - accuracy: 0.5072 - auc: 0.1883 - val_loss: 2.0320 - val_accuracy: 0.4578 - val_auc: 0.1549\n",
      "Epoch 116/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7986 - accuracy: 0.5070 - auc: 0.1890 - val_loss: 2.0306 - val_accuracy: 0.4578 - val_auc: 0.1546\n",
      "Epoch 117/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7966 - accuracy: 0.5080 - auc: 0.1893 - val_loss: 2.0291 - val_accuracy: 0.4578 - val_auc: 0.1544\n",
      "Epoch 118/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7947 - accuracy: 0.5083 - auc: 0.1894 - val_loss: 2.0277 - val_accuracy: 0.4578 - val_auc: 0.1543\n",
      "Epoch 119/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7928 - accuracy: 0.5083 - auc: 0.1902 - val_loss: 2.0266 - val_accuracy: 0.4582 - val_auc: 0.1542\n",
      "Epoch 120/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7909 - accuracy: 0.5094 - auc: 0.1905 - val_loss: 2.0252 - val_accuracy: 0.4586 - val_auc: 0.1545\n",
      "Epoch 121/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7891 - accuracy: 0.5094 - auc: 0.1903 - val_loss: 2.0240 - val_accuracy: 0.4582 - val_auc: 0.1556\n",
      "Epoch 122/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7873 - accuracy: 0.5090 - auc: 0.1906 - val_loss: 2.0227 - val_accuracy: 0.4586 - val_auc: 0.1551\n",
      "Epoch 123/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7855 - accuracy: 0.5095 - auc: 0.1908 - val_loss: 2.0215 - val_accuracy: 0.4590 - val_auc: 0.1561\n",
      "Epoch 124/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7838 - accuracy: 0.5097 - auc: 0.1911 - val_loss: 2.0203 - val_accuracy: 0.4582 - val_auc: 0.1556\n",
      "Epoch 125/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7821 - accuracy: 0.5101 - auc: 0.1914 - val_loss: 2.0191 - val_accuracy: 0.4590 - val_auc: 0.1553\n",
      "Epoch 126/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7805 - accuracy: 0.5107 - auc: 0.1916 - val_loss: 2.0180 - val_accuracy: 0.4594 - val_auc: 0.1554\n",
      "Epoch 127/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7789 - accuracy: 0.5106 - auc: 0.1923 - val_loss: 2.0170 - val_accuracy: 0.4594 - val_auc: 0.1555\n",
      "Epoch 128/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7774 - accuracy: 0.5108 - auc: 0.1924 - val_loss: 2.0158 - val_accuracy: 0.4594 - val_auc: 0.1553\n",
      "Epoch 129/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7759 - accuracy: 0.5116 - auc: 0.1927 - val_loss: 2.0148 - val_accuracy: 0.4598 - val_auc: 0.1554\n",
      "Epoch 130/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7743 - accuracy: 0.5118 - auc: 0.1935 - val_loss: 2.0138 - val_accuracy: 0.4598 - val_auc: 0.1556\n",
      "Epoch 131/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7729 - accuracy: 0.5125 - auc: 0.1930 - val_loss: 2.0128 - val_accuracy: 0.4598 - val_auc: 0.1562\n",
      "Epoch 132/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7715 - accuracy: 0.5126 - auc: 0.1930 - val_loss: 2.0117 - val_accuracy: 0.4598 - val_auc: 0.1566\n",
      "Epoch 133/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7700 - accuracy: 0.5122 - auc: 0.1933 - val_loss: 2.0108 - val_accuracy: 0.4594 - val_auc: 0.1565\n",
      "Epoch 134/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7687 - accuracy: 0.5121 - auc: 0.1936 - val_loss: 2.0099 - val_accuracy: 0.4594 - val_auc: 0.1574\n",
      "Epoch 135/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7673 - accuracy: 0.5131 - auc: 0.1939 - val_loss: 2.0090 - val_accuracy: 0.4594 - val_auc: 0.1575\n",
      "Epoch 136/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7660 - accuracy: 0.5134 - auc: 0.1940 - val_loss: 2.0081 - val_accuracy: 0.4594 - val_auc: 0.1577\n",
      "Epoch 137/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7647 - accuracy: 0.5137 - auc: 0.1946 - val_loss: 2.0072 - val_accuracy: 0.4598 - val_auc: 0.1585\n",
      "Epoch 138/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7635 - accuracy: 0.5144 - auc: 0.1950 - val_loss: 2.0063 - val_accuracy: 0.4598 - val_auc: 0.1588\n",
      "Epoch 139/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7622 - accuracy: 0.5138 - auc: 0.1950 - val_loss: 2.0055 - val_accuracy: 0.4598 - val_auc: 0.1586\n",
      "Epoch 140/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7611 - accuracy: 0.5143 - auc: 0.1950 - val_loss: 2.0046 - val_accuracy: 0.4598 - val_auc: 0.1587\n",
      "Epoch 141/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7599 - accuracy: 0.5154 - auc: 0.1955 - val_loss: 2.0039 - val_accuracy: 0.4598 - val_auc: 0.1588\n",
      "Epoch 142/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7587 - accuracy: 0.5152 - auc: 0.1957 - val_loss: 2.0031 - val_accuracy: 0.4601 - val_auc: 0.1588\n",
      "Epoch 143/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7575 - accuracy: 0.5154 - auc: 0.1960 - val_loss: 2.0023 - val_accuracy: 0.4601 - val_auc: 0.1588\n",
      "Epoch 144/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7564 - accuracy: 0.5152 - auc: 0.1959 - val_loss: 2.0016 - val_accuracy: 0.4601 - val_auc: 0.1597\n",
      "Epoch 145/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7554 - accuracy: 0.5157 - auc: 0.1959 - val_loss: 2.0008 - val_accuracy: 0.4598 - val_auc: 0.1599\n",
      "Epoch 146/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7543 - accuracy: 0.5163 - auc: 0.1960 - val_loss: 2.0001 - val_accuracy: 0.4598 - val_auc: 0.1598\n",
      "Epoch 147/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7533 - accuracy: 0.5163 - auc: 0.1965 - val_loss: 1.9994 - val_accuracy: 0.4601 - val_auc: 0.1591\n",
      "Epoch 148/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7523 - accuracy: 0.5166 - auc: 0.1962 - val_loss: 1.9986 - val_accuracy: 0.4605 - val_auc: 0.1591\n",
      "Epoch 149/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7512 - accuracy: 0.5166 - auc: 0.1967 - val_loss: 1.9981 - val_accuracy: 0.4605 - val_auc: 0.1593\n",
      "Epoch 150/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7502 - accuracy: 0.5167 - auc: 0.1970 - val_loss: 1.9973 - val_accuracy: 0.4605 - val_auc: 0.1592\n",
      "Epoch 151/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7493 - accuracy: 0.5171 - auc: 0.1970 - val_loss: 1.9967 - val_accuracy: 0.4613 - val_auc: 0.1592\n",
      "Epoch 152/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7483 - accuracy: 0.5169 - auc: 0.1977 - val_loss: 1.9961 - val_accuracy: 0.4613 - val_auc: 0.1592\n",
      "Epoch 153/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7474 - accuracy: 0.5177 - auc: 0.1978 - val_loss: 1.9955 - val_accuracy: 0.4621 - val_auc: 0.1593\n",
      "Epoch 154/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7465 - accuracy: 0.5171 - auc: 0.1979 - val_loss: 1.9949 - val_accuracy: 0.4625 - val_auc: 0.1597\n",
      "Epoch 155/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7456 - accuracy: 0.5178 - auc: 0.1980 - val_loss: 1.9943 - val_accuracy: 0.4625 - val_auc: 0.1600\n",
      "Epoch 156/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7448 - accuracy: 0.5178 - auc: 0.1981 - val_loss: 1.9937 - val_accuracy: 0.4628 - val_auc: 0.1580\n",
      "Epoch 157/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7440 - accuracy: 0.5182 - auc: 0.1981 - val_loss: 1.9932 - val_accuracy: 0.4625 - val_auc: 0.1579\n",
      "Epoch 158/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7431 - accuracy: 0.5178 - auc: 0.1981 - val_loss: 1.9926 - val_accuracy: 0.4632 - val_auc: 0.1578\n",
      "Epoch 159/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7423 - accuracy: 0.5182 - auc: 0.1982 - val_loss: 1.9921 - val_accuracy: 0.4640 - val_auc: 0.1577\n",
      "Epoch 160/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7415 - accuracy: 0.5184 - auc: 0.1983 - val_loss: 1.9916 - val_accuracy: 0.4640 - val_auc: 0.1579\n",
      "Epoch 161/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7407 - accuracy: 0.5191 - auc: 0.1985 - val_loss: 1.9910 - val_accuracy: 0.4640 - val_auc: 0.1580\n",
      "Epoch 162/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7399 - accuracy: 0.5187 - auc: 0.1990 - val_loss: 1.9905 - val_accuracy: 0.4640 - val_auc: 0.1581\n",
      "Epoch 163/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7392 - accuracy: 0.5193 - auc: 0.1994 - val_loss: 1.9900 - val_accuracy: 0.4636 - val_auc: 0.1582\n",
      "Epoch 164/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7384 - accuracy: 0.5194 - auc: 0.1992 - val_loss: 1.9895 - val_accuracy: 0.4640 - val_auc: 0.1583\n",
      "Epoch 165/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7377 - accuracy: 0.5191 - auc: 0.1992 - val_loss: 1.9890 - val_accuracy: 0.4644 - val_auc: 0.1585\n",
      "Epoch 166/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7370 - accuracy: 0.5193 - auc: 0.1993 - val_loss: 1.9886 - val_accuracy: 0.4644 - val_auc: 0.1585\n",
      "Epoch 167/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7363 - accuracy: 0.5203 - auc: 0.1995 - val_loss: 1.9881 - val_accuracy: 0.4644 - val_auc: 0.1584\n",
      "Epoch 168/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7357 - accuracy: 0.5200 - auc: 0.1996 - val_loss: 1.9877 - val_accuracy: 0.4640 - val_auc: 0.1584\n",
      "Epoch 169/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7350 - accuracy: 0.5200 - auc: 0.1998 - val_loss: 1.9872 - val_accuracy: 0.4644 - val_auc: 0.1585\n",
      "Epoch 170/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7343 - accuracy: 0.5205 - auc: 0.1997 - val_loss: 1.9868 - val_accuracy: 0.4644 - val_auc: 0.1585\n",
      "Epoch 171/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7337 - accuracy: 0.5202 - auc: 0.1998 - val_loss: 1.9864 - val_accuracy: 0.4644 - val_auc: 0.1587\n",
      "Epoch 172/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7331 - accuracy: 0.5202 - auc: 0.1999 - val_loss: 1.9860 - val_accuracy: 0.4644 - val_auc: 0.1588\n",
      "Epoch 173/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7325 - accuracy: 0.5196 - auc: 0.2002 - val_loss: 1.9856 - val_accuracy: 0.4644 - val_auc: 0.1588\n",
      "Epoch 174/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7319 - accuracy: 0.5202 - auc: 0.2001 - val_loss: 1.9852 - val_accuracy: 0.4644 - val_auc: 0.1610\n",
      "Epoch 175/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7313 - accuracy: 0.5203 - auc: 0.2003 - val_loss: 1.9848 - val_accuracy: 0.4644 - val_auc: 0.1610\n",
      "Epoch 176/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7307 - accuracy: 0.5204 - auc: 0.2003 - val_loss: 1.9844 - val_accuracy: 0.4648 - val_auc: 0.1611\n",
      "Epoch 177/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7301 - accuracy: 0.5206 - auc: 0.2002 - val_loss: 1.9840 - val_accuracy: 0.4644 - val_auc: 0.1610\n",
      "Epoch 178/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7296 - accuracy: 0.5209 - auc: 0.2003 - val_loss: 1.9837 - val_accuracy: 0.4648 - val_auc: 0.1610\n",
      "Epoch 179/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7290 - accuracy: 0.5210 - auc: 0.2005 - val_loss: 1.9833 - val_accuracy: 0.4644 - val_auc: 0.1614\n",
      "Epoch 180/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7285 - accuracy: 0.5210 - auc: 0.2006 - val_loss: 1.9830 - val_accuracy: 0.4648 - val_auc: 0.1615\n",
      "Epoch 181/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7280 - accuracy: 0.5210 - auc: 0.2005 - val_loss: 1.9826 - val_accuracy: 0.4648 - val_auc: 0.1615\n",
      "Epoch 182/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7275 - accuracy: 0.5211 - auc: 0.2009 - val_loss: 1.9823 - val_accuracy: 0.4644 - val_auc: 0.1615\n",
      "Epoch 183/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7270 - accuracy: 0.5214 - auc: 0.2010 - val_loss: 1.9819 - val_accuracy: 0.4644 - val_auc: 0.1620\n",
      "Epoch 184/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7265 - accuracy: 0.5215 - auc: 0.2010 - val_loss: 1.9816 - val_accuracy: 0.4652 - val_auc: 0.1620\n",
      "Epoch 185/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7260 - accuracy: 0.5214 - auc: 0.2010 - val_loss: 1.9813 - val_accuracy: 0.4648 - val_auc: 0.1620\n",
      "Epoch 186/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7256 - accuracy: 0.5215 - auc: 0.2011 - val_loss: 1.9810 - val_accuracy: 0.4652 - val_auc: 0.1621\n",
      "Epoch 187/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7251 - accuracy: 0.5214 - auc: 0.2011 - val_loss: 1.9807 - val_accuracy: 0.4652 - val_auc: 0.1604\n",
      "Epoch 188/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7246 - accuracy: 0.5215 - auc: 0.2012 - val_loss: 1.9804 - val_accuracy: 0.4652 - val_auc: 0.1599\n",
      "Epoch 189/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7242 - accuracy: 0.5219 - auc: 0.2016 - val_loss: 1.9801 - val_accuracy: 0.4648 - val_auc: 0.1599\n",
      "Epoch 190/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7238 - accuracy: 0.5218 - auc: 0.2018 - val_loss: 1.9798 - val_accuracy: 0.4652 - val_auc: 0.1599\n",
      "Epoch 191/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7233 - accuracy: 0.5218 - auc: 0.2020 - val_loss: 1.9796 - val_accuracy: 0.4652 - val_auc: 0.1599\n",
      "Epoch 192/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7229 - accuracy: 0.5223 - auc: 0.2022 - val_loss: 1.9793 - val_accuracy: 0.4648 - val_auc: 0.1600\n",
      "Epoch 193/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7225 - accuracy: 0.5224 - auc: 0.2023 - val_loss: 1.9790 - val_accuracy: 0.4655 - val_auc: 0.1600\n",
      "Epoch 194/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7221 - accuracy: 0.5220 - auc: 0.2025 - val_loss: 1.9788 - val_accuracy: 0.4655 - val_auc: 0.1600\n",
      "Epoch 195/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7217 - accuracy: 0.5224 - auc: 0.2024 - val_loss: 1.9785 - val_accuracy: 0.4655 - val_auc: 0.1598\n",
      "Epoch 196/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7213 - accuracy: 0.5228 - auc: 0.2024 - val_loss: 1.9783 - val_accuracy: 0.4652 - val_auc: 0.1598\n",
      "Epoch 197/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7210 - accuracy: 0.5225 - auc: 0.2026 - val_loss: 1.9780 - val_accuracy: 0.4655 - val_auc: 0.1598\n",
      "Epoch 198/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7206 - accuracy: 0.5229 - auc: 0.2026 - val_loss: 1.9778 - val_accuracy: 0.4655 - val_auc: 0.1599\n",
      "Epoch 199/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7202 - accuracy: 0.5228 - auc: 0.2027 - val_loss: 1.9776 - val_accuracy: 0.4652 - val_auc: 0.1600\n",
      "Epoch 200/200\n",
      "379/379 [==============================] - 2s 5ms/step - loss: 1.7199 - accuracy: 0.5232 - auc: 0.2028 - val_loss: 1.9773 - val_accuracy: 0.4652 - val_auc: 0.1600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb6439bdd10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# late fusion model training\n",
    "periphery_model.fit(x=periphery_train, y=labels_train, epochs=200, validation_data=(periphery_val, labels_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d6f291d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 0s 3ms/step - loss: 1.9481 - accuracy: 0.4694 - auc: 0.1488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.948124647140503, 0.4693877696990967, 0.14883194863796234]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "periphery_model.evaluate(periphery_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbfe609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model.compile(loss='categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(learning_rate=lr), metrics=['accuracy', tf.keras.metrics.AUC(curve='PR')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2852633b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 22:38:20.758955: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1824098304 exceeds 10% of free system memory.\n",
      "2021-12-09 22:38:21.747708: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1824098304 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-09 22:38:23.870954: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379/379 [==============================] - 34s 62ms/step - loss: 8.6119 - accuracy: 0.0853 - auc_1: 0.0125 - val_loss: 4.3529 - val_accuracy: 0.1201 - val_auc_1: 0.0125\n",
      "Epoch 2/10\n",
      "379/379 [==============================] - 22s 59ms/step - loss: 4.3385 - accuracy: 0.1095 - auc_1: 0.0125 - val_loss: 4.3236 - val_accuracy: 0.1201 - val_auc_1: 0.0125\n",
      "Epoch 3/10\n",
      "379/379 [==============================] - 22s 58ms/step - loss: 4.3098 - accuracy: 0.1146 - auc_1: 0.0125 - val_loss: 4.2955 - val_accuracy: 0.1201 - val_auc_1: 0.0125\n",
      "Epoch 4/10\n",
      "379/379 [==============================] - 22s 59ms/step - loss: 4.2828 - accuracy: 0.1146 - auc_1: 0.0125 - val_loss: 4.2691 - val_accuracy: 0.1201 - val_auc_1: 0.0125\n",
      "Epoch 5/10\n",
      "379/379 [==============================] - 22s 58ms/step - loss: 4.2567 - accuracy: 0.1146 - auc_1: 0.0351 - val_loss: 4.2431 - val_accuracy: 0.1201 - val_auc_1: 0.0449\n",
      "Epoch 6/10\n",
      "379/379 [==============================] - 22s 59ms/step - loss: 4.1417 - accuracy: 0.1146 - auc_1: 0.0379 - val_loss: 4.1300 - val_accuracy: 0.1201 - val_auc_1: 0.0387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb63ea42d90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# late fusion model training\n",
    "image_model.fit(x=image_train, y=labels_train, epochs=10, validation_data=(image_val, labels_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9fcebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation \n",
    "image_prediction = image_model.predict(image_test)\n",
    "periphery_prediction = periphery_model.predict(periphery_test)\n",
    "late_fusion_prediction = 0.999*periphery_prediction + 0.001*image_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ec46839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 1s 16ms/step - loss: 4.1260 - accuracy: 0.1336 - auc_1: 0.0385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.125993251800537, 0.13361571729183197, 0.03847469761967659]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_model.evaluate(image_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "565c2061",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=np.argmax(late_fusion_prediction, axis=1)\n",
    "y_test=np.argmax(labels_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b9ec1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13059860396199002"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb273945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46938775510204084"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74d18476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_model accuracies\n",
      "0.13361571043511744\n",
      "0.01282051282051282\n"
     ]
    }
   ],
   "source": [
    "print(\"image_model accuracies\")\n",
    "print(accuracy_score(y_test, np.argmax(image_prediction, axis=1)))\n",
    "print(balanced_accuracy_score(y_test, np.argmax(image_prediction, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b75719c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "periphery_model accuracies\n",
      "0.46938775510204084\n",
      "0.13059860396199002\n"
     ]
    }
   ],
   "source": [
    "print(\"periphery_model accuracies\")\n",
    "print(accuracy_score(y_test, np.argmax(periphery_prediction, axis=1)))\n",
    "print(balanced_accuracy_score(y_test, np.argmax(periphery_prediction, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2da5fdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_arr = np.bincount(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "37f58157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "count_arr = np.bincount(y_test)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb4702d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = []\n",
    "for i in range(0, 80):\n",
    "    if str(i) in report:\n",
    "        f1_scores.append(report[str(i)]['f1-score'])\n",
    "    else:\n",
    "        f1_scores.append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdf6b6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb4cc19f650>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(count_arr, f1_scores)\n",
    "plt.xlabel('Number of Dta Points in Class')\n",
    "plt.ylabel('f1-score')\n",
    "m, b = np.polyfit(count_arr, f1_scores, 1)\n",
    "plt.plot(count_arr, m*count_arr + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360fea8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
